{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BAAJg31ij1L"
   },
   "source": [
    "# <center> Майнор \"Интеллектуальный анализ данных\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_J6UXd1ij1Y"
   },
   "source": [
    "# <center> Курс \"Современные методы машинного обучения\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYtVG13Jij1b"
   },
   "source": [
    "# <center> Лабораторная работа №2. Object Detection. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POmNlHez5xWQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO8D4VK7ij1d"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rcdM-buhij1f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7uDmELAij1l"
   },
   "source": [
    "Install library for processing the labeling\n",
    "```bash\n",
    "pip install xmltodict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q8CWga7d_XcJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in c:\\users\\polina\\anaconda3\\lib\\site-packages (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4cFYT77dij1n"
   },
   "outputs": [],
   "source": [
    "import xmltodict, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrE0tJX0ij1r"
   },
   "source": [
    "В этом задании потребуется обучить детектор фруктов на изображении. Данные находятся в архиве `data.zip`. Данные уже поделены на train и test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FYn7wqiDF_z6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wJQFd9cij1x"
   },
   "source": [
    "Датасет для трех классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HYiR9cEqij19"
   },
   "outputs": [],
   "source": [
    "class2tag = {\"apple\": 0, \"orange\": 1, \"banana\": 2}\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        self.transform = transform\n",
    "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
    "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
    "            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n",
    "            with open(annotation) as f:\n",
    "                annotation_dict = xmltodict.parse(f.read())\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
    "            if not isinstance(objects, list):\n",
    "                objects = [objects]\n",
    "            for obj in objects:\n",
    "                bndbox = obj[\"bndbox\"]\n",
    "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
    "                bbox = list(map(int, bbox))\n",
    "                bboxes.append(torch.tensor(bbox))\n",
    "                labels.append(class2tag[obj[\"name\"]])\n",
    "            self.annotations.append(\n",
    "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.transform:\n",
    "            # the following code is correct if you use albumentations\n",
    "            # if you use torchvision transforms you have to modify it =)\n",
    "            res = self.transform(\n",
    "                image=self.images[i],\n",
    "                bboxes=self.annotations[i][\"boxes\"],\n",
    "                labels=self.annotations[i][\"labels\"],\n",
    "            )\n",
    "            return res[\"image\"], {\n",
    "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
    "                \"labels\": torch.tensor(res[\"labels\"]),\n",
    "            }\n",
    "        else:\n",
    "            return self.images[i], self.annotations[i]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ9A2Vsxij2A"
   },
   "source": [
    "<br>  \n",
    "<br>  \n",
    "Функции для вычисления mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4kNeTMzwij2C"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(dt_bbox, gt_bbox):\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "\n",
    "    intersection_bbox = np.array(\n",
    "        [\n",
    "            max(dt_bbox[0], gt_bbox[0]),\n",
    "            max(dt_bbox[1], gt_bbox[1]),\n",
    "            min(dt_bbox[2], gt_bbox[2]),\n",
    "            min(dt_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
    "        intersection_bbox[3] - intersection_bbox[1], 0\n",
    "    )\n",
    "    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "\n",
    "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
    "    gt_bboxes = target_true[\"boxes\"].numpy()\n",
    "    gt_labels = target_true[\"labels\"].numpy()\n",
    "\n",
    "    dt_bboxes = target_pred[\"boxes\"].numpy()\n",
    "    dt_labels = target_pred[\"labels\"].numpy()\n",
    "    dt_scores = target_pred[\"scores\"].numpy()\n",
    "\n",
    "    results = []\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "\n",
    "        detection_result_dict = {\"score\": dt_score}\n",
    "\n",
    "        max_IoU = 0\n",
    "        max_gt_id = -1\n",
    "        for gt_id in range(len(gt_labels)):\n",
    "            gt_bbox = gt_bboxes[gt_id, :]\n",
    "            gt_label = gt_labels[gt_id]\n",
    "\n",
    "            if gt_label != dt_label:\n",
    "                continue\n",
    "\n",
    "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
    "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
    "                max_gt_id = gt_id\n",
    "\n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            detection_result_dict[\"TP\"] = 1\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict[\"TP\"] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, targets_true) in enumerate(test_loader):\n",
    "            images = list(image.to(device).float() for image in images)\n",
    "            targets_pred = model(images)\n",
    "            targets_true = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n",
    "            ]\n",
    "            targets_pred = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n",
    "            ]\n",
    "\n",
    "            for i in range(len(targets_true)):\n",
    "                target_true = targets_true[i]\n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
    "\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "\n",
    "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "    acc_TP = np.zeros(len(results))\n",
    "    acc_FP = np.zeros(len(results))\n",
    "    recall = np.zeros(len(results))\n",
    "    precision = np.zeros(len(results))\n",
    "\n",
    "    if results[0][\"TP\"] == 1:\n",
    "        acc_TP[0] = 1\n",
    "    else:\n",
    "        acc_FP[0] = 1\n",
    "\n",
    "    for i in range(1, len(results)):\n",
    "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
    "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
    "\n",
    "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
    "        recall[i] = acc_TP[i] / nbr_boxes\n",
    "\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAhZRzEEij2I"
   },
   "source": [
    "<br>  \n",
    "<br>  \n",
    "  \n",
    "## Часть 1.  \n",
    "  \n",
    "*Вес в общей оценке - 0.4*  \n",
    "  \n",
    "В данной части вам нужно решить задачу детектирования фруктов \"без классификации\". Для этого все три класса нужно объединить в один (нужно внести соотвествующие изменения в датасет)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggbYZdKYij2M"
   },
   "source": [
    "### Задание 1.  \n",
    "  \n",
    "Примените обученные детекторы из [torchvision.models](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection) - Faster R-CNN, RetinaNet, SSD (можно только с одним backbone, можно все попробовать) - и оцените качество детекции на тестовом датасете. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ETdjTuh_ymJh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations==0.4.6\n",
      "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from albumentations==0.4.6) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\polina\\anaconda3\\lib\\site-packages (from albumentations==0.4.6) (1.5.2)\n",
      "Collecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\polina\\anaconda3\\lib\\site-packages (from albumentations==0.4.6) (5.3.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from albumentations==0.4.6) (4.5.4.60)\n",
      "Requirement already satisfied: six in c:\\users\\polina\\anaconda3\\lib\\site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.17.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\polina\\anaconda3\\lib\\site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (8.0.1)\n",
      "Collecting Shapely\n",
      "  Downloading Shapely-1.8.0-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\polina\\anaconda3\\lib\\site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.3.2)\n",
      "Requirement already satisfied: imageio in c:\\users\\polina\\anaconda3\\lib\\site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2020.6.20)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
      "Building wheels for collected packages: albumentations\n",
      "  Building wheel for albumentations (setup.py): started\n",
      "  Building wheel for albumentations (setup.py): finished with status 'done'\n",
      "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65168 sha256=ef4db0784292914ad9ce9362e6046fe0653f61804925ccc51145358f7212c585\n",
      "  Stored in directory: c:\\users\\polina\\appdata\\local\\pip\\cache\\wheels\\d2\\e3\\0b\\99a239413035502833a7b07283894243fddf5ce3aa720ca8dd\n",
      "Successfully built albumentations\n",
      "Installing collected packages: Shapely, imgaug, albumentations\n",
      "Successfully installed Shapely-1.8.0 albumentations-0.4.6 imgaug-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ui5EnEqayvp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-1.5.5-py3-none-any.whl (525 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (20.4)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "Collecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
      "Collecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (2.7.0)\n",
      "Requirement already satisfied: torch>=1.7.* in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (1.9.0+cu111)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (4.50.2)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\polina\\anaconda3\\lib\\site-packages (from pytorch_lightning) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\polina\\anaconda3\\lib\\site-packages (from packaging>=17.0->pytorch_lightning) (1.15.0)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-win_amd64.whl (555 kB)\n",
      "Requirement already satisfied: requests; extra == \"http\" in c:\\users\\polina\\anaconda3\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.24.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (50.3.1.post20201107)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.35.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp38-cp38-win_amd64.whl (122 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Using cached charset_normalizer-2.0.9-py3-none-any.whl (39 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp38-cp38-win_amd64.whl (83 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (20.3.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp38-cp38-win_amd64.whl (45 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from requests; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from requests; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from requests; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from requests; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\polina\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\users\\polina\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\polina\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
      "Installing collected packages: multidict, yarl, charset-normalizer, frozenlist, aiosignal, async-timeout, aiohttp, fsspec, torchmetrics, pyDeprecate, pytorch-lightning\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.8.3\n",
      "    Uninstalling fsspec-0.8.3:\n",
      "      Successfully uninstalled fsspec-0.8.3\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 charset-normalizer-2.0.9 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.5 torchmetrics-0.6.1 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RT_bYvByjC4X"
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "from sklearn.metrics import auc\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import datasets, models\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "N53Y0AbhhfSR"
   },
   "outputs": [],
   "source": [
    "def get_transform(train=False):\n",
    "    transforms_list = []\n",
    "    transforms_list.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms_list.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uiF9T6qZyJI0"
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(height=800, width=800),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "IfC9flragZSL"
   },
   "outputs": [],
   "source": [
    "train_dataset = FruitDataset(\"/data/train\", transform=transform) # get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Jk0-Ogyuf9dS"
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-69c3e34c6ed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81iXv9UtiLvZ"
   },
   "outputs": [],
   "source": [
    "test_dataset = FruitDataset(\"/data/train\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bho8ELm-icPC"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "\n",
    "    :param batch: an iterable of N sets from getitem()\n",
    "    :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "    \"\"\"\n",
    "\n",
    "    images = list()\n",
    "    targets = list()\n",
    "\n",
    "    images, targets = [], []\n",
    "    for i, t in batch:\n",
    "        images.append(i)\n",
    "        target = {'boxes': t['boxes'], 'labels': t['labels']}\n",
    "        targets.append(target)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return {'images': images, 'targets': targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4588K_JDjUPP"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONpQNkYUOygA"
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ7t-Szc2lD8"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3s0AEtj5gGaC"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    global_loss = 0\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(data_loader), leave=False, desc=\"Batch number\"):\n",
    "        # for images, targets in tqdm(batch.values(), leave=False, desc=\"Batch number\"):\n",
    "        images = batch['images']\n",
    "        targets = batch['targets']\n",
    "        print(images)\n",
    "        print(targets)\n",
    "        # images = list(image.to(device).float() for image in images)\n",
    "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # print(targets)\n",
    "        dict_loss = model(images, targets)\n",
    "        print('dl', dict_loss)\n",
    "        losses = sum(loss for loss in dict_loss.values())\n",
    "        print('los', losses)\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        global_loss += float(losses.cpu().detach().numpy())\n",
    "        print('gl', global_loss)\n",
    "    return global_loss\n",
    "\n",
    "# def train_one_epoch(model, optimizer, data_loader, device):\n",
    "#     model.train()\n",
    "#     global_loss = 0\n",
    "#     for images, targets in tqdm(data_loader, leave=False, desc=\"Batch number\"):\n",
    "        \n",
    "#         images = list(image.to(device).float() for image in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         dict_loss = model(images, targets)\n",
    "#         losses = sum(loss for loss in dict_loss.values())\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         global_loss += float(losses.cpu().detach().numpy())\n",
    "\n",
    "#     return global_loss\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, device, num_epochs=5):\n",
    "    for epoch in trange(num_epochs, leave=True, desc=f\"Epoch number\"):\n",
    "        train_one_epoch(model, optimizer, train_loader, device)\n",
    "        mAP = evaluate(model, test_loader, device=device)\n",
    "\n",
    "        print(f\"mAP after epoch {epoch + 1} is {mAP:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JlgrRWXi4s4"
   },
   "outputs": [],
   "source": [
    "def get_detection_model(num_classes=2):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKf6H1p7jSTI"
   },
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "\n",
    "model = get_detection_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "train(model, train_loader, test_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xif5C9uMij2P"
   },
   "source": [
    "### Задание 2.  \n",
    "  \n",
    "Обучите детекторы из задания выше на обучающем датасете, оцените качество на тестовом. При необходимости, подберите гиперпараметры - optimizer, lr, weight_decay etc.  \n",
    "Выполните обучение в двух вариантах: со случайной инициализацией весов и с загрузкой весов уже обученной модели. Сравните качество. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZwLwnq0ij2Q"
   },
   "source": [
    "### Задание 3.  \n",
    "  \n",
    "- Для лучшей модели оцените, как меняется качество на тестовых данных при изменении порога IoU.  \n",
    "- Также добавьте порог для минимального значения score у предсказанных bounding box'ов, таким образом отсеивая предсказания с низким конфиденсом. Оцените, как меняется качество при изменении порога (порог IoU используйте 0.5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxQolyEdij2S"
   },
   "source": [
    "### Задание 4.  \n",
    "  \n",
    "Нарисуйте предсказанные bounding box'ы для любых двух картинок из __тестового__ датасета и каких-нибудь картинок из __интернета__ (релевантных - где есть эти фрукты - и тех, где этих фруктов нет)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sByvAEkaij2T"
   },
   "source": [
    "### Задание 5.  \n",
    "  \n",
    "Реализуйте и примените Non-maximum Suppression. Оцените (визуально и по метрикам), как его использование влияет на качество детекции.   \n",
    "**NB:** Чтобы продемонстрировать эффективность применения алгоритма, возможно, нужно будет взять достаточно низкий порог конфиденса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBWBc9IAij2V"
   },
   "source": [
    "<br>  \n",
    "<br>  \n",
    "  \n",
    "## Часть 2.  \n",
    "  \n",
    "*Вес в общей оценке - 0.4*  \n",
    "  \n",
    "Выполните задания 2-5 из предыдущей части для случая 3-х классов (как в изначальном датасете)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3o1nRpoij2W"
   },
   "source": [
    "<br>  \n",
    "<br>  \n",
    "  \n",
    "## Часть 3.  \n",
    "  \n",
    "*Вес в общей оценке - 0.2*  \n",
    "  \n",
    "Обучите модель для object detection для трех классов на __обучающем__ датасете и добейтесь PR AUC не менее __0.91__ на  __тестовом__.  \n",
    "Баллы за задание вычисляются по формуле: __min(2, 2 * Ваш auc / 0.91)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCw1c5ukij2X"
   },
   "source": [
    "<br>  \n",
    "<br>  \n",
    "  \n",
    "## Бонусные задания.  \n",
    "  \n",
    "1. При обучении используйте аугментации (в первую очередь пространственные) из [albumentations](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/) или [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html).  \n",
    "2. Возьмите одну из [детекционных архитектур](https://paperswithcode.com/sota/object-detection-on-coco) (желательно, не старее 2020 года), у которой выложены тренировочный код и чекпоинты на гитхабе, обучите и провалидируйте ее на данных этой лабораторной. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Modern_ML.Lab_2.Object_Detection.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
